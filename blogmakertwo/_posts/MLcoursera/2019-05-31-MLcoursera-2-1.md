---
layout: post
current: post
cover:  assets/images/ml.jpg
navigation: True
title: ML Coursera 2-1 Linear Regression with Multiple Variables
date: 2019-05-31 10:00:00
tag :
  - machine learning
  - Coursera
class: post-template
subclass: 'post tag-getting-started'
author: hy38
---

> Coursera Andrew Ng 교수님의 ML 강의를 정리한 포스트입니다.

# Linear Regression with Multiple Variables

## Intro
안녕하세요. 이번 포스트에서는 multiple variables(features)를 가진 선형회귀에 대해 알아보겠습니다.
지난 포스트에서 하나의 variable을 가지는 선형회귀를 공부했는데, 그 연장선이라고 보시면 됩니다.
우리는 지도학습에서 선형회귀, 분류를 공부한 다음 비지도학습 등 더욱 다양한 것들을 공부할 것입니다. 따라서 이 포스트 다음으로는 기본적인 Octave 설명을 다루고, 곧바로 분류(Classification)을 공부하게 되겠습니다.

## Notation

![multiple_features_data](/assets/images/ml/coursera/week2/multiple-features-data.png)

먼저, 위와 같은 데이터를 봐주세요. 이전의 포스트에서는 단순히 집의 크기에 대한 가격을 예상하는 선형회귀를 배웠다면, 이번에는 더욱 구체적으로 들어가서 다양한 features를 기반으로 집가격을 예상하게 됩니다. 그에 앞서서 표기법에 대해 정리하겠습니다.

  - x의 밑첨자는 각각의 feature들을 의미합니다. 예를들어 침실의 개수에 관한 데이터는 $x_2$입니다.
  - y는 이전과 같이 output variable, 즉 집가격을 의미합니다.
  - m : 이전에 설명한것과 같이 example의 갯수를 말합니다. 위의 테이블에서는 각각의 row들이 각 집들의 데이터이기 때문에, 총 4개의 집 example이 있어 m은 4입니다.
  - n : features의 갯수들입니다. 위 테이블에서는 크기, 침실갯수, 몇층인지, 몇년되었는지 해서 총 4개의 feature가 있네요.
  - **$x^i$** : 하나의 example에 대한 vector입니다. 쉽게 말해서 하나의 row를 뜻합니다. $x^3$은 세번째 row인 [1534 3 2 30] 벡터를 가리키게 됩니다.
  - **$x_j^i$** : i번째 row data의 j번째 feature의 값 을 가리킵니다. 이를테면 $x_4^3$은 30을 가리키겠죠?
  
## Changed Hypothesis
기존의 단일변수 선형회귀에서는 다음과 같은 가설함수를 이용했습니다.
$$h_\theta(x) = \theta_0 + \theta_1x $$
그러나, 더 많은 features들이 생겨남에 따라 이 식을 변경시킬 필요성이 생깁니다.
결과적으로 우리는 다음과 같은 가설함수를 세우게 됩니다.
$$h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + \theta_4x_4$$

여기서 $x_0$을 1로 설정하게 된다면, $$h_\theta(x) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + \theta_4x_4$$ 가 위의 식과 동일해 보이게 될 것입니다. 다만, $x_0$라는 feature가 추가로 생겨나게 되면서, **feature vector**의 **dimension**은 n에서 n+1 로 커집니다. **parameter**인 $\theta$의 **dimension**은 기존과 같은 n+1이 됩니다. 이로써 두 **column vector**들의 **dimension**이 동일해졌습니다.

다시 이 가설함수를 볼까요?
$$h_\theta(x) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + \theta_4x_4$$
이것을 선형대수를 이용하여 간단하게 표현하면,
$$h_\theta(x) = \theta^TX$$
로 표현할 수 있습니다..!! 엄청난 식의 간소화를 진행한 셈이죠!
이 때 $h_\theta(x)$의 **dimension**은 [1 x n+1] * [n+1 x 1] = [1 x 1]이 됩니다.

## Gradient Descent for Multiple Variables
가설함수 설정을 마친 우리는, 이제 경사하강법(Gradient Descent)를 적용해볼 것입니다. 이에 앞서 다음과 같이 개별적인 $\theta$들로 이루어진 우리의 cost function J함수 $$J(\theta_0, \theta_1, ... , \theta_n)$$ 의 parameter을 dimension이 n+1인 $\theta$ **벡터**로 바꾸어주도록 합니다.

![multiple_features_cost_function](/assets/images/ml/coursera/week2/multiple-features-cost-function.png)

마찬가지로 우리의 gradient descent algorithm또한 parameter을 변경해주도록 합시다.

![multiple_features_gradient_descent](/assets/images/ml/coursera/week2/multiple-features-gradient-descent.png)

이제 이 알고리즘은 다음과 같이 표기될 수 있습니다.

![multiple_features_gradient_descent_new](/assets/images/ml/coursera/week2/multiple-features-gradient-descent-new.png)

위 식의 끝에 $x^{(j)}_i$가 붙은것은 편미분의 결과로 해석됩니다.

이로써 gradient descent가 multiple variables에 대하여 어떻게 달라지는지에 대해 간단하게 살펴보았습니다.

아래의 식은 굉장히 중요한 역할을 함을 기억하면 좋을것 같습니다.

![gradient_descent_partial_derivative](/assets/images/ml/coursera/week2/gradient-descent-partial-derivative.png)

## Feature Scaling
하나가 아닌 multiple variable를 갖는 문제에서는 이 variable들이 너무나 큰 **range** 차이가 나게되면 gradient descent를 진행할 경우 다음과 같이 진행하게 됩니다. 이렇게 진행하게 될 경우에는 궁극적인 global minimum에 도달하기까지 오랜 시간이 걸리게 됩니다.

![gradient_descent_bad](/assets/images/ml/coursera/week2/gradient-descent-bad.png)

따라서 우리는 빠른 global minimum으로의 수렴을 위해 variables / features 들이 비슷한 range를 갖도록 다듬어주는 feature scaling이라는 작업을 진행하게 됩니다.

feature scaling의 결과는 놀랍습니다. 타원이었던 $J(\theta)$ 그래프의 이심율이 점차 작아지면서 원의 형태를 띄게 됩니다.

가장 좋은 범주는 $ -1 \leq x_(i) \leq 1 $ 혹은 $ -0.5 \leq x_(i) \leq 0.5 $ 라고 합니다.

### Mean Normalization
feature scaling의 technique 중 하나인 mean normalization을 이용하여 range들을 일정하게 통일시키는 방법을 보여드리겠습니다.

![mean_normalization](/assets/images/ml/coursera/week2/mean-normalization.png)

위 식을 이용하여 mean normalization을 진행하게 됩니다. 그렇다면 우리가 [다음과 같은 문제](https://stats.stackexchange.com/questions/157923/feature-scaling-and-mean-normalization)에서 normalized된 $x_2^{(4)}$를 구하는 것을 보여드리겠습니다. 위 식의 $x_i$에 $x_2^{(4)}$를 대입하고, $\mu_i$에 **mean** 즉 평균을 대입하고, $s_i$에는 최댓값에서 최솟값을 뺀 **range**값을 대입하거나 표준편차를 대입하여 진행합니다.(분명 range 대입시와 표준편차 대입시의 결과는 다릅니다.)

위 링크의 문제에서 $s_i = 8836 − 4761 = 4075$이고, $x_i = 4761$, $\mu_i = 6675.5$이므로, 정답 $x_2^{(4)}$는 -0.47입니다.
이 때에는 $s_i$를 구하기 위하여 **range**를 채택하였습니다.

## Gradient Descent Tips
다음은 경사하강법을 진행할 때의 팁입니다.
learning rate $\alpha$를 채택하는 

### Plot Debugging
x축에 iteration의 횟수, y축에 $J(\theta)$를 plot으로 그려보면, 다음 그림과 같이 더이상 감소하지 않는 구간이 생깁니다. 

![plot_debugging](/assets/images/ml/coursera/week2/plot-debugging.png)

물론 y축인 $J(\theta)$가 항상 감소하게끔 너무 큰 $\alpha$를 잡지 않도록 주의해야합니다. $\alpha$가 너무 큰 경우에는 overshooting이 발생할 수 있습니다. 따라서 적당한 $\alpha$를 설정해야합니다. 이때의 팁은 알파를 3배씩 증감시켜주면서 조절하는 것입니다. 예를들어 0.01, 0.03, 0.003 등과 같이 말입니다. 이들 0.01, 0.03, 0.003 등의 alpha에 대한 plot들을 비교해보면서 적절한 alpha를 찾아내는 과정이 중요하겠습니다.

### Automatic Convergence Test
iteration이 유의미한 차이를 더이상 만들어내지 못할 때에 iteration을 중지시키고 최소가 되는 $J(\theta)$를 구하는 방법 또한 존재합니다. 다만, 이 방법의 아쉬운 점은 유의미한 차이인 threshold를 찾아내는것이 쉽지 않다는 것입니다. 이 threshold가 적절하지 않으면, 한참 내려가고 있을 때에 iteration을 중지시키게 될 수도 있습니다.
따라서 plot을 활용하여 x값이 일직선이 이루어지는 $J(\theta)$를 찾아내는 방법이 보편적입니다.

## Choice of Features and Polynomial Regression

### Choosing the most appropriate Feature
우리가 집값을 예측함에 있어서 불필요한 feature를 사용하고 있을지도 모릅니다. 예를들어 집의 넓이를 중요한 feature로 생각하며 집을 구하고있는데, 주어진 feature이 고작 가로($x_1$)와 세로($x_2$) 길이(length)라면, **새로운 feature을 만들어서 사용할 수도 있습니다.** 새로운 feature는 $x_1 * x_2$(가로 * 세로)이겠죠. 이렇게 하면 2개의 feature을 1개로 줄일 수 있게됩니다. 그리고 이는 다음과 같은 가설함수를 설정할 수 있어보입니다.
$$h_\theta(x) = \theta_0 + \theta_1x_3 $$
이를 통해 알 수 있듯이, 새로운 feature을 만들어서 사용하게되면 좋은 모델을 만들어 낼 수 있습니다.

### Polynomial Regression
처음에 polynomial regression을 linear regression에서 접하였을 때에 문득 무언가가 이상하다는 생각이 들었습니다. 선형회귀를 하는데, polynomial regression이 갑자기 튀어나와 당황했었습니다. 아래 링크는 linear regression이 parameter에 있어 linear함을 설명해줍니다. 참고해주세요.

++ [why polynomial regression is in the category of linear regression](https://www.quora.com/Why-is-linear-regression-called-linear-if-we-can-use-quadratic-equations-in-our-model-which-gives-a-curved-line)

우리가 linear regression으로 데이터를 fitting하는데에 한계를 느낄때에는 polynomial regression을 이용할 수 있습니다. 이 때 우리의 가설함수 $$h_\theta(x) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3$$에서의  $x_1$은 $x$, $x_2$는 $x^2$, $x_3$은 $x^3$로 매핑시켜주면 됩니다.

![polynomial_regression](/assets/images/ml/coursera/week2/polynomial-regression.png)

위와 같은 데이터의 경우에는 꼭지점 이후에 내려가는 오목한 2차 다항식보다는 3차 다항식이 더 잘 들어맞는것같죠?
3차다항식 이외에도 제곱근 함수 $\sqrt{x}$($x^{1\over2}$)를 이용하면 더 적절한 데이터 피팅이 될 것 같습니다.

## Normal Equation
여태까지 우리는 linear regression문제들을 해결하기 위해 gradient descent를 이용하여 데이터에 가장 잘 들어맞는 가설함수를 찾아나갔습니다.
하지만, gradient descent 외에도 **normal equation**을 이용하여 데이터를 피팅할 수 있습니다. 살짝 복잡하긴 하지만, 많지 않은 데이터를 다룰때에는 gradient descent보다 빠르게 산술적으로 $\theta$값들을 구하게 됩니다.

정규방정식을 이용한 해를 구하는 식은 다음과 같습니다.

![normal_equation](/assets/images/ml/coursera/week2/normal-equation.png)

위 식에 대한 자세한 풀이는 좋은 강의영상으로 대체하겠습니다.

++ [정규방정식을 이용한 해 구하기](https://www.edwith.org/linearalgebra4ai/lecture/24131/)


그렇다면 이제 정규방정식을 이용하여 주어진 데이터에 맞는 $\theta$값을 구해보도록 하겠습니다. 우선, 데이터는 이번 포스트 시작할 때 사용되었던 multiple features가 있는 집값예측 데이터입니다.

![multiple_features_data](/assets/images/ml/coursera/week2/multiple-features-data.png)

정규방정식에서의 X는 design matrix로써, 모든 feature정보를 [m x n+1] matrix에 담습니다. n+1은 $x_0$ feature의 column을 추가한 것 입니다.마찬가지로, 정규방정식에서의 y는 [m x 1] matrix의 column vector로 정의됩니다.

위의 데이터를 이용하여 정규방정식에 대입하게 되면 다음과 같습니다.

![normal_equation_2](/assets/images/ml/coursera/week2/normal-equation-2.png)

위 식을 계산하면 cost function을 최소화하는, 즉 데이터에 가장 적합한 $\theta$값을 구하게 됩니다.
한번 계산해볼까요?

![normal_equation_result](/assets/images/ml/coursera/week2/normal-equation-result.png)



그렇다면, 언제 어떤 경우에 **gradient descent**를 쓰고, 어떨때 **normal equation**을 써야할까요?
먼저 **Gradient Descent**의 장단점을 나열해보겠습니다.

  - 장점
    - 엄청난 양의 데이터를 다룰 때 빠르다.
      - 엄청난 양이라 함은 n이 10000보다 큰 경우를 뜻한다.

  - 단점
    - learning rate $\alpha$를 정해야한다.
    - iteration을 사용하기 때문에 속도가 느리다.($O(kn^2)$)
    
    
반면에, **Normal Equation**의 장단점은 다음과 같습니다.

  - 장점
    - learning rate $\alpha$를 정할 필요가 없으며, convergence를 체크할 필요가 없다.
    - iteration을 사용하지 않기 때문에 속도가 빠른 편이다.
  
  - 단점
    - 역행렬을 연산할 필요가 있으며, 이는 $O(n^3)$의 시간복잡도를 갖는다.
    - n이 커질수록 연산속도가 느려진다.

**따라서, 보통 n이 10000을 넘지 않는 경우에는 간단하게 normal equation을 이용하여 cost function을 최소화하는 $\theta$값을 구하고,
n이 10000보다 큰 엄청난 양의 데이터의 경우에는 gradient descent를 이용하여 적절한 $\theta$값을 구합니다.**

### Normal Equation and non-invertibility
선형대수를 공부하셨기 때문에 정규방정식에서 쓰이는 역행렬이 **존재하지 않으면** 어쩌지라는 생각을 하셨을 수도 있습니다.
다행이도, MATLAB이나 octave의 경우 **pinv** 라는 pseudo inverse function이 존재하여 non-invertible한 matrix에 대해서도 올바른 값을 계산해줍니다.

그렇다면 마지막으로 $X^TX$가 non-invertible하다는 것이 갖는 의미에 대해 알아보겠습니다. 일반적으로는 두 가지 원인이 있습니다.

첫째는 불필요한 feature들이 존재할 경우입니다.
두번째는 너무 많은 feature들이 존재할 경우입니다. 예를들어, feature의 개수 n이 m보다 훨씬 많은 경우에 $X^TX$가 가끔 비가역성인 경우가 생기게됩니다. 너무 많은 feature들은 추후에 배울 **[regularization](https://hy38.github.io/ml/MLcoursera-3-2/)**을 이용하여 문제를 해결하거나, feature들을 지워나가면서 해결할 수 있습니다.

또한, linearly dependent한 feature을 지우는 방법도 있다고 합니다.

지금까지 Linear Regression with Multiple Variables 에 대해 알아보았습니다. 다음 시간에는 Octave / Matlab Tutorial을 건너뛰고 **Logistic Regression**에 대해 포스팅하겠습니다.
오탈자 / 오류 / 궁금사항 댓글로 남겨주시면 적극 반영하여 더욱 발전된 모습을 보여드리겠습니다. 궁금하신 것이 있으시면 언제든지 댓글을 남겨주시기 바랍니다. 감사합니다.
