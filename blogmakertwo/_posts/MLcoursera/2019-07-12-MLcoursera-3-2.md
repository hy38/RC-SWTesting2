---
layout: post
current: post
cover:  assets/images/ml.jpg
navigation: True
title: ML Coursera 3-2 Regularization
date: 2019-07-12 10:00:00
tag :
  - machine learning
  - Coursera
class: post-template
subclass: 'post tag-getting-started'
author: hy38
---

> Coursera Andrew Ng 교수님의 ML 강의를 정리한 포스트입니다.

# Regularization

이전 포스트를 끝으로 머신러닝의 회귀와 분류에 대해 알아보았습니다. 이제 우리는 **Regularization** 라고 하는 정규화에 대해 알아보겠습니다.

Regularization이란, 근본적으로 **Overfitting**을 해결하기 위한 노력입니다. 그렇다면 오버피팅이란 무엇일까요? 단어를 뜯어보면, 
over(과하게, 많이) + fitting(피팅되다) 정도로 볼 수 있겠네요! 그렇습니다. 우리가 가설함수를 설정할 때 학습데이터를 너무 잘 **끼워맞추기** 위해
과하게 fitting하는 것이 오버피팅입니다. 여기서 "과하게" 라는것은 가설함수를 너무 복잡하게 설정한다는 것을 의미합니다. 그런데 여기서 학습데이터를
잘 맞추면 좋은것 아닌가요? 라고 할 수 있겠지만, 아쉽게도 오버피팅의 본질적인 문제점은 학습데이터 **"만"** 잘 맞추고, 시험용데이터나 실전에서는 낮은
정확도를 보인다는 것입니다.반대로 **Underfitting** 또한 존재합니다. 언더피팅은 너무 간단한 가설함수를 설정하면 발생하는 문제입니다. 
이들을 그림으로 확인하면 다음과 같습니다.

![over_under_fitting_image](/assets/images/ml/coursera/week3/over-under-fitting-image.png)

위의 linear regression가설함수가 복잡해짐에 따라 학습용데이터가 잘 fitting되는것을 확인할 수 있습니다. 물론 다음과 같은 Logistic Regression에서도 마찬가지로 발생합니다.

![over_under_fitting_logistic_image](/assets/images/ml/coursera/week3/over-under-fitting-logistic-image.png)


## Addressing Overfitting

오버피팅을 다루는 방법은 크게 3가지가 있습니다.
- 첫째, 더 많은 학습데이터를 이용하여 가설함수를 훈련한다.
- 둘째, feature의 수를 줄인다.
  - 이 경우 수동으로 feature들을 제거하는데, data loss를 최소화하는 지워질 feature들을 고른다. 물론 이럼에도 data loss는 발생한다.
- 셋째, Regularization을 이용한다.
  - 모든 feature들을 남겨두지만, 매개변수 $\theta$의 크기를 감소시킨다. 이는 각각 조금씩 y결과값 예측에 기여하는 feaeture가 많을 때 유용하다.

이중에서 우리는 세번째 방법인 Regularization을 이용하여 Overfitting을 해결해볼 것입니다. 
위의 오버피팅된 linear regression 집값예측 그래프를 Regularize하는 것을 설명하겠습니다.
먼저, $\theta$의 크기를 줄이기 위해서 cost function을 다음과 같이 수정합니다. $\theta_3$와 $\theta_4$의 크기를 줄이기(penalize) 위함인데요, 

![modified_cost_function](/assets/images/ml/coursera/week3/modified-cost-function.png)

$\theta_3$와 $\theta_4$의 크기가 많이 줄게 되면 다음 두 항 $\theta_3 x^3$와 $\theta_4 x^4$의 영향력이 거의 사라집니다. 그럼 결국에는 2차다항식만이
남아있는것처럼 보이게됩니다.

![linear_regression_regularization](/assets/images/ml/coursera/week3/linear-regression-regularization.png)

cost function에 큰 계수가 붙은 theta를 추가하였을뿐인데, 이것이 어떤 방식으로 theta의 크기를 감소시키는것일까요?

Cost function은 우리가 최소화해야하는 함수입니다. 그 함수에 엄청나게 큰 계수를 달고있는 $\theta$들을 포함시키게 되면, 기존과 같은 값의 theta들로는
cost function J를 키우기만 할 뿐, 줄이는데 성공하지 못할것입니다. 따라서 J를 minimize하는 $\theta$들 중 큰 계수를 달고 들어온 penalize당하는 
새로운 항들($\theta$)은 자연스럽게 크기가 작아질 수 밖에 없게됩니다. J를 최소화해야하기 때문입니다.

이로인해 우리는 더욱 간단한 2차다항식의 형태의 hypothesis function을 얻게됩니다.

정리하면 다음과 같습니다.
- penalize위해 Cost function에 항들을 추가함.
- Cost function을 minimize하는 과정에서 $\theta_3$와 $\theta_4$의 크기가 줄어들게됨.
- 그러다보니 간단한 hypothesis가 완성되는것처럼 됨.

이번에는, 특정 $\theta$를 penalize하는것이 아닌, $\theta_0$을 제외한 다른 모든 $\theta$들을 penalize하는 방법입니다. 이는 어떤 항들을 penalize해야할지를 고르지 못하는 상황에서 사용합니다.
따라서 이 경우의 항들이 추가된 Cost function은 다음과 같습니다.

$$ J(\theta) = {1 \over 2m} * [\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2 + \lambda\sum_{i=1}^n\theta_i^2] $$

관습적으로 $\theta_0$는 penalize하지 않는다고 합니다. 사실 $\theta_0$를 penalize하더라도 유의미한 차이가 없다고 하네요. 위와같은 식을 사용하게 되면 이전보다 부드러운 곡선이 등장하게됩니다. data fitting도 잘되고 hypothesis function도 훨씬 좋아집니다.

위 식에서 새로 보는 문자가 하나 등장합니다. 바로 $\lambda$인데요, 이를 **regularization parameter**라고 합니다. 우리는 이 $\lambda$를 조절하며 얼마나 regularize할지를 정하는데요, 이 람다가 0이라면, 기존의 일반적인 cost function이 만들어지며, 이는 overfitting이 유지가됩니다. 만일, 람다가 200같이 너무 큰 수로 지정되면, 반대로 세타들을 너무 penalize하게되어 underfitting이 발생하게 됩니다. 이보다 더 큰 수를 람다로 지정하게 되면, penalize하지 않는 $\theta_0$를 제외한 모든 $\theta$들이 거의 없는듯한 효과를 보이게 되므로 $h_\theta(x) = \theta_0$인 상수함수만 남은것처럼 보이게 됩니다.
따라서 regularization parameter $\lambda$를 잘 선택하는것이 중요합니다.

## Regularized Linear Regression

위에서 regularization을 이용한 linear regression의 cost function $J(\theta)$에 대해 살펴보았습니다. 이전과 마찬가지로 $J(\theta)$를 minimize하는 $\theta$를 찾아야합니다. 이때, gradient descent를 사용하는데, 우선 이전의 기존 linear regression의 gradient descent식은 다음과 같습니다.

![regularized_linear_gradient](/assets/images/ml/coursera/week3/regularized-linear-gradient.png)

여기서 $\theta_0$를 구별해준 것을 알 수 있습니다. $\theta_0$를 penalize하지 않기 때문에 구별한것입니다. 

위 식의 $\theta_j$ 식에 다음과 같이 ${\lambda \over m}\theta_j$을 추가해줍니다. 참고로 이 식은 cost function $J(\theta)$의 편미분 결과입니다.

이 떄, 위 $\theta_j$의 update 식을 $\theta_j(1 - \alpha{\lambda \over m})$로 묶어낼 수 있습니다. 
여기서 다음 항을 주목할 필요가 있습니다. 

$$ (1 - \alpha{\lambda \over m}) $$

주로 alpha는 작은수, m은 큰 수이기 때문에, 위 항은 1보다 작게됩니다. 주로 0.95 ~ 0.99 사이의 값들을 갖게 되는데, 이 수치들을 $\theta_j$에 매 update시마다 곱해주게 됩니다. 그 이외의 항은 기존 gradient descent와 동일합니다.

이를 통해 보다 직관적으로 $\theta_j$를 매 update마다 감소시킴을 알 수 있습니다. $\theta$값들의 penalize가 이렇게 작동합니다.

### Regularization with the Normal Equation

$J(\theta)$를 minimize하기위한 방법중 하나인 normal equation을 이용한 방법으로도 regularization을 진행할 수 있습니다. 기존의 linear regression의 normal equation식은 다음과 같았습니다.

$$ \theta = (X^TX)^{-1}X^Ty $$

기억이 안나시면 [여기](/ml/MLcoursera-2-1/)를 참고해주세요!

위 기존 normal equation에서 다음과 같이 항을 추가하여 regularization을 진행하게 됩니다. 

![normal_equation_regularization](/assets/images/ml/coursera/week3/normal-equation-regularization.png)

위 추가된 항인 $\lambda[matrix]$에서 행렬의 첫번째 원소가 0인 이유는 j=0일 때 $\theta_j$를 penalize하지 않기 위함입니다.

Normal Equation을 이용할 때 항상 고려하게 되는것이 바로 **Non-invertibility**인데요, 딱히 고려할 필요가 없어집니다.
그 이유는, $(X^TX + \lambda[matrix])$가 항상 invertible하기 때문입니다. 

따라서, 기존의 $X^TX$항이 $m \le n$ 이면 invertible하다는 조건을 따질 필요가 없어졌습니다. 여러모로 regularization을 이용하면 좋아지네요!


## Regularization for Logistic Regression

마지막으로 Logistic Regression에서의 Regularization에 대해 살펴보겠습니다.
Logistic Regression을 regularize하는 과정이 Linear Regression과 비슷합니다.

먼저 cost function $J(\theta)$에 다음 항을 추가합니다.

$$ +{\lambda \over 2m}\sum_{j=1}^n\theta_j^2 $$

완성된 $J(\theta)$는 다음과 같습니다.

![regularized_logistic_cost_function](/assets/images/ml/coursera/week3/regularized-logistic-cost-function.PNG)

추가된 항의 sum이 j=1부터 시작됨을 유의해주세요. 이는 j=0일 때의 $\theta_0$를 penalize하지 않겠다는 뜻입니다.

이제 cost function J에 gradient descent를 적용해보겠습니다.

기존의 logistic regression의 $\theta$ update rule은 다음과 같았습니다.

![original_logistic_gradient_descent](/assets/images/ml/coursera/week3/original-logistic-gradient-descent.png)

여기서 linear regression과 동일한 방법으로 다음과 같은 식을 유도할 수 있습니다.

![modified_logistic_gradient_descent](/assets/images/ml/coursera/week3/modified-logistic-gradient-descent.png)

위 $\theta$ update rule이 regularized linear regression과 동일하게됨을 알 수 있습니다. 
**다만, 가설함수가 다르기 때문에, 식만 같을뿐이지, 실제로 동일하지는 않습니다.**
