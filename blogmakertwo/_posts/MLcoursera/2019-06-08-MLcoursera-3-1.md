---
layout: post
current: post
cover:  assets/images/ml.jpg
navigation: True
title: ML Coursera 3-1 Logistic Regression
date: 2019-06-08 10:00:00
tag :
  - machine learning
  - Coursera
class: post-template
subclass: 'post tag-getting-started'
author: hy38
---

> Coursera Andrew Ng 교수님의 ML 강의를 정리한 포스트입니다.

[Introduction](/ml/MLcoursera-1-1/)에서 언급했듯이 **지도학습**은 크게 회귀(Regression)와 분류(Classification)로 나눌 수 있습니다.
이전까지 우리는 회귀에 대해 간단히 알아보았습니다. 다음으로 분류에 대해 알아볼 차례입니다.

# Logistic Regression

우리는 이제 Classification 알고리즘인 logistic regression에 대해 살펴보겠습니다. 다만 이에앞서, 많은 분들이 왜 Classification을 공부하는데 regression을 배우는지에 대해 궁금하실 수 있다고 생각합니다. 물론 저도 그랬었고요. "logistic **REGRESSION**"이라고 이름이 붙여진 것은 역사상의 이유라고 합니다. 따라서 이는 기본적으로 분류를 위한 알고리즘이며, 회귀라는 이름이 있다고 해서 회귀라고 생각하면 안될 것 같습니다. 이에 대한 보다 자세한 설명은 아래 링크에 나와있습니다.

++ [Why is logistic regression called regression](https://www.quora.com/Why-is-logistic-regression-called-regression-if-it-doesnt-model-continuous-outcomes)

분류에서 기본적으로 y는 discrete한 값이어야 합니다. 예를 들어, 어떤 이메일이 스팸인지(1) 아닌지(0), 혹은 어떤 종양이 악성인지(1) 양성인지(0)와 같이 0, 1, 2, 3, ... (혹은 1, 2, 3, 4, ...등 이산적인 값들을 y에 사용합니다.

먼저, binary class classification problem을 살펴보겠습니다. 사실 binary와 multiclass classification problem은 거의 비슷합니다. 다만, 마지막에 one vs all classification을 적용하게 되는데, 이는 뒤에서 다시 살피도록 하겠습니다.

## linear regression to a Classification Problem?

![linear_regression_to_classification_problem](/assets/images/ml/coursera/week3/linear-regression-to-classification-problem.png)

선형회귀에 분류를 적용하려면, 일정 값 이상이면 1, 미만이면 0 등의 분류 임계값이 필요한데, 이를 **threshold**라고 합니다. 위 그림에서는 threshold가 0.5임을 알 수 있습니다. 

위 그림처럼 이전에 공부한 선형회귀를 분류에도 적용시킬 수 있다면 너무 편할것 같습니다. 하지만 아쉽게도 몇가지 문제점들이 발생하게 됩니다. 한번 분류문제에 선형회귀를 시도해보면서 그 문제점들을 파악해보겠습니다.

  - 1. 위 그림만 보면 얼핏 잘 fitting되는것처럼 보이기도 합니다. 하지만 위 데이터셋에 매우 작은 종양 크기의 Malignant한 종양이 발견된다면 자칫 많은 데이터들이 이상하게 분류될 수 있는 상황이 발생합니다.

  - 2. 우리가 예측하는 y값은 0 혹은 1 등의 값이지만, linear regression을 위한 hypothesis function이 주는 결과값은 0보다 작을 수도, 1보다 클 수도 있습니다. 따라서 우리는 0 혹은 1의 값을 갖도록 만들어줄 필요가 있을 것입니다.

## Hypothesis Representation

### 가설함수의 조건

우리가 분류에 사용할 가설함수 $h_\theta(x)$는 0과 1사이의 값을 가지면 좋을 것 같습니다. 다음과 같이 말입니다.

$$ 0 \leq h_\theta(x) \leq 1$$

이를 만족하는 함수가 있는데, 바로 **sigmoid function** 입니다. 이 함수의 식은 ${1 \over  1+e^{-x}}$ 입니다.
[Desmos](https://www.desmos.com/calculator/bgontvxotm)를 이용하여 그려보면, 다음과 같은 그래프를 띕니다.

![desmos_sigmoid_graph](/assets/images/ml/coursera/week3/desmos-sigmoid-graph.PNG)

우리는 이 식을 이용하여 가설함수를 세울 것입니다. 이 때 기존의 선형회귀 가설함수 식인 $$h_\theta(x) = \theta^Tx$$ 에 g라는 함수를 합성한 함수인 $$h_\theta(x) = g(\theta^Tx)$$ 라는 식을 얻어서 이 식의 $\theta^Tx$를 $z$로 치환하는 과정을 거칩니다. 그 결과 다음과 같은 함수가 나옵니다.

$$ h_\theta(x) = g(z) = {1 \over  1+e^{-z}} $$

### 가설함수 output값의 해석

우리가 세운 가설함수 h(x)가 숫자를 반환할 때, 우리는 이를 **해당 x가 input으로 들어갔을 때 y=1이 되는 확률**이라 합니다.
예를들어 주어진 x 값이 어떤 환자의 의료데이터(종양의 크기, 모양 등)라 했을 때, output이 h(x) = 0.83 이라고 나오게 된다면, 이 환자의 종양이 악성일 확률은 83%가 되는 것 입니다.

이러한 것을 식으로 표현하면 다음과 같습니다.

$$h_\theta(x) = P(y=1 | x; \theta)$$

또한, 우리가 다루는 이진분류(Binary Classification)에서 다음이 성립합니다.

$$P(y=1 \vert x; \theta) + P(y=0 \vert x; \theta) = 1$$ 

우리가 갖게 될 y값은 0 아니면 1이기 때문이죠.


## Decision Boundary

우리의 가설함수 h(x) = g(z) 는 시그모이드 함수 입니다. h(x)가 0.5 이상일 경우 y=1, 0.5 미만일 경우 y=0 이라고 분류를 하게 되는 것입니다.
이 때, g(z)가 0.5 이상인 부분은 그래프상에서 확인 가능하듯이 **z가 양수**인 경우입니다. 우리는 z를 $\theta^Tx$로 정의했으므로, 다음과 같을 경우에 h(x)가 0.5보다 커지며, 이는 y=1이라는 output을 만들어냅니다.

if $ \theta^Tx \ge 0 $  then $h_\theta(x) \ge 0.5,$ $y=1$

따라서, $ \theta^Tx \ge 0.5 $ 일 때 $y = 1$ 이 성립합니다. 우리는 이를 이용해 Decision Boundary라는 경계를 만들어낼 수 있습니다.

추가로, g(z)가 0.5인 경우에는 주로 y=1로 판정하기는 하지만, 그 판정이 틀릴확률이 높다는 것을 염두해야 할 것 같습니다. 


본격적으로, 다음과 같은 가설함수에, 데이터셋이 주어졌을 경우의 분류를 생각해봅시다.

$$ h_\theta(x) = g(z) = g(\theta_0 + \theta_1 x_1 + \theta_2 x_2) = g(\theta^Tx) $$

![decision_boundary_data](/assets/images/ml/coursera/week3/decision-boundary-data.png)

위와 같은 단순한 데이터셋을 분류하려면 어떤 방법이 있을까요?
상식적으로는, 둘 사이에 직선을 긋는 방법이 있을 것 같습니다.

하지만, 컴퓨터는 상식을 모릅니다. 따라서, 직선을 긋게되는 원칙과 방법을 알려주어야 합니다.

h(x)가 y=1 이라는 결과값을 도출해내는 과정의 중심에는 $\theta^Tx \ge 0$ 이 있었습니다. 따라서, 이 $\theta^Tx \ge 0$, 즉 $z$가 0보다 크거나 같은 경우는 1, 작은 경우는 0을 분류해내도록 만들면 됩니다.

위 가설함수에서 $z$는 $\theta_0 + \theta_1 x_1 + \theta_2 x_2$ 입니다. 우리는 이 $\theta$값들에 다음과 같은 값을 정해줘보겠습니다.

$$ \theta_0 = -3, \theta_1 = 1, \theta_2 = 1 $$

위 경우 $z$는 **y=1일 때** 다음과 같은 부등식이 성립합니다.

$$ -3x_0 + 1x_1 + 1x_2 \ge 0 $$

그리고 위 식을 정리하면, 고등학교 수학시간에 배운 단순한 "부등식의 영역" 문제가 됩니다.
저 영역을 만족하는 x들의 경우 y=1이 되는것이지요. 그림으로 그리면, 다음과 같습니다.

![decision_boundary](/assets/images/ml/coursera/week3/decision-boundary.png)

다음과 같이 두 데이터를 가르는 직선을 긋는 방법이 있겠죠.
이 때, 이 직선이 바로 **Decision Boundary** 입니다.

신기한 점은, 이 Decision Boundary는 가설함수와 매개변수만을 이용하여 세운것이며, 어떠한 데이터도 사용되지 않았다는 점입니다. 우리는 데이터를 적절한 매개변수를 찾는데 사용하게 되는데, 이렇게 해서 얻어진 매개변수가 위 그림의 경우 $\theta^T = [-3, 1, 1]$ 이라고 보시면 될 것 같습니다. 

### Non-linear Decision Boundaries

Decision Boundary는 linear하지 않을수도 있습니다. 다음과 같은 Decision Boundary도 존재합니다.

![decision_boundary_nonlinear](/assets/images/ml/coursera/week3/decision-boundary-nonlinear.png)

위 경우에는 가설함수 $h_\theta(x)$가 다음과 같습니다.

$$ h_\theta(x) = g(\theta_0 + \theta_1 x_1 + \theta_2 x_1^2 + \theta_4 x_2^2 $$

이 가설함수에 데이터셋을 이용하여 가장 적절한 매개변수를 찾아보았더니 $\theta^T = [-1, 0, 1, 1]$ 였고, 이를 통해 우리는 $x_1^2 + x_2^2 \ge 1$ 이라는 반지름이 1인 원의 바깥부분에 해당하는 x값들이 y=1을 output으로 준다는 것을 알 수 있습니다.

추가적으로, 원보다 더욱 고차원의 복잡한 decision boundary를 만드는 것 또한 가능합니다. 이 때는 $z$를 복잡하게 설정해주면 되겠죠?

## Cost Function for Logistic Regression

Cost function의 본질은 결국 가장 적절한 매개변수 $\theta$를 찾아내는 것입니다. 이는 cost가 가장 적은 parameter를 채택하면서 이루어집니다. 앞선 linear regression이 그랬듯이, logistic regression에서도 본질은 변하지 않습니다.

linear regression에서의 cost function $J(\theta)$는 다음과 같았습니다.

$$ J(\theta) = {1 \over m}\sum_{i=1}^m {1 \over 2} (h_\theta(x^{(i)}) - y^{(i)})^2  $$

이해의 편의를 위해서 logistic regression에서는, linear regression의 cost인 $ {1 \over 2} (h_\theta(x^{(i)}) - y^{(i)})^2  $을 통째로 $ Cost(h_\theta(x), y) $로 바꾸어봅시다. Cost function의 기능(m개 데이터의 cost들의 합)이 똑같이 유지되도록 말이죠. J를 바꾼 결과 다음과 같은 식이 만들어집니다.

$$ J(\theta) = {1 \over m}\sum_{i=1}^m Cost(h_\theta(x), y) $$

이렇게까지 식을 변형하는 이유는, 기존의 Squared Error Function에 logistic regression의 가설함수 h(x) = g(z) 를 대입하면 문제가 생기기 때문인데요. linear regression의 가설함수 h(x)의 경우 **linearity**, 즉 선형이기 때문에 결과적으로 **convex**한 function이었습니다. 하지만, logistic regression의 가설함수 h(x) = g(z)의 경우, 시그모이드 함수 $ h_\theta(x) = g(z) = {1 \over  1+e^{-z}} $가 비선형; 즉 선형결합으로 표현될 수 없기 때문에, **non-convex**한 function이 됩니다.

구체적으로 **non-convex**함을 확인해보면 다음과 같습니다. 먼저, 가설함수 g(z)가 비선형이기 때문에

$$ Cost(h_\theta(x), y) = {1 \over 2} (h_\theta(x) - y)^2 = {1 \over 2}({1 \over  1+e^{-\theta^Tx}} - y)^2 $$

위 식을 다시 $J(\theta)$에 대입하여 $\theta$와 $J(\theta)$에 대한 그래프를 plot 해보게 되면 다음과 같은 모양이 나옵니다.

![too_many_local_minimums](/assets/images/ml/coursera/week3/too-many-local-minimums.png)

Cost function J가 수많은 local minimum을 갖게되며, 이는 Gradient Descent를 이용하여 **global minimum**을 찾는것을 어렵게합니다.

따라서 가설함수가 **convex**한지 여부는 굉장히 중요합니다. 이에 따라 우리는 $ Cost(h_\theta(x), y) $를 **convex**한 식으로 변경할 것입니다. 새로운 Cost는 다음과 같습니다.

![convex_logistic_regression_cost](/assets/images/ml/coursera/week3/convex-logistic-regression-cost.png)

이것이 우리가 logistic regression에서 사용할 cost이고, 따라서 우리의 cost function $J(\theta)$는 다음과 같습니다.

![convex_logistic_regression_cost_function](/assets/images/ml/coursera/week3/convex-logistic-regression-cost-function.png)

여기서 추가로 y가 1이든 0이든 관계없이 간소화된(simplified) 하나의 식으로 병합을 할 수 있는데, 다음과 같습니다.

![simplified_cost_function](/assets/images/ml/coursera/week3/simplified-cost-function.png)

이는 y가 무조건 0 혹은 1이기에 가능한 것 입니다.


Cost Function J에 대한 소개는 마쳤고, 이 함수에 Gradient Descent를 적용하기 이전에, Cost Function에 대해 어떻게 저 함수가 나왔는지에 대한 이해를 하고 넘어가겠습니다. 이것은 다시 다음 식에서 시작됩니다.

![convex_logistic_regression_cost](/assets/images/ml/coursera/week3/convex-logistic-regression-cost.png)

Cost라 함은 기본적으로 우리가 세운 가설의 결과값과 실제 결과데이터값의 **차이**를 의미합니다. 가설함수가 y=0을 예측했는데 y=1이 정답이었다면, 엄청나게 큰 비용, 즉 $ \infty $ Cost 를 초래하겠죠. 반대로, 가설함수가 y=1을 예측했는데, 정답 역시 예측한대로 y=1이라면, 0의 비용을 초래할 것입니다. 이러한 것들을 잘 설명해주는 함수가 $ -log(h_\theta(x)) $입니다. 다음과 같은 y=-log(x)그래프를 띄며, 방금 내용이 쉽게 이해가 갈 것입니다.

![cost_when_y_equals_one](/assets/images/ml/coursera/week3/cost-when-y-equals-one.png)

y=0일때의 경우를 생각해보세요! 비슷한 방식으로 접근하면 이해하실 수 있으실 것입니다.

## Minimizing Cost Function with Gradient Descent

이제껏 구해온 Cost Function J를 이제는 최소화할 차례입니다.

가장 적은 cost를 갖는 $\theta$를 찾은 후 가설함수에 테스트 데이터 $x$를 넣어서 나오는 결과값이 바로 $h_\theta(x)$이며 이것은 y=1일 확률을 나타내줍니다.

이전에 linear regression의 Gradient Descent는 다음과 같았습니다.

![cost_when_y_equals_one](/assets/images/ml/coursera/week2/multiple-features-gradient-descent-new.png)

Logistic Regression의 Gradient Descent 또한 위와 같습니다..!
이는 logistic regression의 cost function $J(\theta)$를 편미분해보면 알 수 있는데요, 다음과 같은 과정을 거칩니다.

![partial_derivative_1](/assets/images/ml/coursera/week3/partial-derivative-1.png)

![partial_derivative_2](/assets/images/ml/coursera/week3/partial-derivative-2.png)

우리는 for-loop을 이용하여 $\theta$를 update할 수 있지만, 더 나은 vectorization을 이용할 수도 있습니다. 다음과 같습니다.

![vectorized_gradient_descent](/assets/images/ml/coursera/week3/vectorized-gradient-descent.png)

## Advanced Optimization

Gradient Descent를 구현할 때에는 다음과 같은 과정을 거칩니다.

1. $J(\theta)$와 그 편미분을 구현한다.
2. 위 둘을 gradient descent식에 대입한다.

이런 Gradient descent보다 향상된 알고리즘이 몇 있습니다. 바로 Confugate gradient, BFGS, L-BFGS입니다. 이들의 장점은 learning rate $\alpha$를 자동으로 잡아주고 속도도 기존의 경사하강법보다 빠르다는 것입니다. 그러한 이유로 커다란 머신러닝 문제(huge feature set)들에 자주 쓰입니다.


## Multiclass Classification Problems

지금까지 두 개의 class만이 존재하는 이진분류(Binary Classification)을 해왔다면, 실제로 많이 쓰일 다중분류에 대해 알아보겠습니다.
다중분류는 이진분류의 연장선이며, 맨 윗부분에서 잠깐 언급한 **one vs all classification**을 이용합니다.

### One vs All Classification

방법은 간단합니다. 각 클래스마다 이진분류를 진행합니다. 다음과 같이 말입니다.

![one_vs_all_classification](/assets/images/ml/coursera/week3/one-vs-all-classification.png)

새로운 데이터 $x$를 받아 $h_\theta(x)^{(i)}$가 최대가 되는 i 클래스를 찾습니다. 그 i 클래스를 채택합니다.

구체적으로는, 각각 학습한 세 개의 이진분류 가설함수들에 새로운 데이터 x를 평가해보라한 뒤, y=1일 그 확률이 가장 큰 클래스를 사용하는 것입니다.


이것으로 Classification에 대한 소개를 마칩니다. 다음 포스트에서는 [Regularization](/ml/MLcoursera-3-2/)에 대해 알아보겠습니다.
