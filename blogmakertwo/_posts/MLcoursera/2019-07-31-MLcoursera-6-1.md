---
layout: post
current: post
cover:  assets/images/ml.jpg
navigation: True
title: ML Coursera 6-1 Advice for Applying Machine Learning
date: 2019-07-31 10:00:00
tag :
  - machine learning
  - Coursera
class: post-template
subclass: 'post tag-getting-started'
author: hy38
---

> Coursera Andrew Ng 교수님의 ML 강의를 정리한 포스트입니다.

# Advice for Applying Machine Learning

지금까지 선형회귀와 분류, 신경망에 대해 알아보았습니다. 이제 이것들을 구현하여 나온 결과값들을 **분석**하여 어떤 문제점들이 있는지, 어떻게 개선해나가야 할지에 대해 살펴보겠습니다.
또한, 모델을 선택하는데에 있어 몇차식(d)을 사용할지와, 정규화 매개변수인 $\lambda$를 어떻게 정할지를 알아보겠습니다.

## Deciding what to try next

시작은 집값을 예측하는 linear regression으로 시작하겠습니다.
현재 상황은 이렇습니다 : 
cost function J를 minimize하는 theta를 구하여(train) 그 theta들을 가지고 새로운 데이터에 적용시켜보았더니(test) 예측결과가 영 꽝인 상황입니다.
이 때 어떤것들을 시도해봐야할까요? 아래는 우리가 시도해볼 수 있는 작업들입니다.

- 더 많은 데이터를 사용한다.
- feature의 수를 줄인다.
- feature의 수를 늘린다.
- polynomial feature을 추가한다.
- regularization parameter $\lambda$를 줄이거나 키워본다.


### Machine Learning Diagnostics

위 작업들 중에서 어떤 경우에 사용해야하며, 어떤 경우에는 효과가 미미한지를 구별해나갈 것입니다.
예를들어, 데이터를 수집하는데에 소요되는 시간이 6개월이지만, 정작 더 많은 데이터가 있어도 모델이 개선되지 않는다면 6개월의 시간낭비를 한 셈일것입니다.
우리는 **진단**하는 법을 배움으로써 **Time saving**을 할 수 있게 될것입니다.


## Evaluation a Hypothesis

우리가 학습을 통해 cost function을 minimize하는 $\theta$를 구할때, 과연 이 cost function의 작은 **error**가 좋은것일지 생각해볼 필요가 있습니다.
너무 적은 error을 갖는 parameter는 **오버피팅**을 의심하게 만듭니다.
이는 곧 parameter가 **generalize**를 잘 하지 못함을 의미하며, **새로운 데이터**에 좋지 않은 결과를 보일것입니다.
$h(\theta)x$를 그려보면 overfitting을 눈대중으로 파악할 수 있겠지만, feature의 개수가 많아질수록 $h(\theta)x$를 그리기가 힘들어지고, 불가능해집니다.

결국 우리의 가설함수 $h(\theta)x$를 **다른 방법으로 Evaluate**할 필요성이 생기게 되는데요, 우리가 가진 데이터셋을 70:30의 비율로 쪼개 training set(70%)과 test set(30%)을 만듭니다. 물론 데이터가 섞이도록 shuffle을 해줘야합니다.

![train_test_split](/assets/images/ml/coursera/week6/train-test-split.png)

### Train and Test Scheme

데이터셋의 분할 이후에는 다음과 같은 순서로 가설함수 $h(\theta)x$를 평가합니다.

1. training set(70%)의 데이터를 이용하여 parameter $\theta$를 학습시킨다.
2. 30%의 남은 데이터를 이용하여 **Test Error**을 구한다.

이 때 Test Error $J_test(\theta)$ 은 다음과 같이 구합니다.
- linear regression:

![J_test_linear_regression](/assets/images/ml/coursera/week6/J-test-linear-regression.png)

- logistic regression: 

![J_test_logistic_regression](/assets/images/ml/coursera/week6/J-test-logistic-regression.png)

이때 logistic regression에서는 더욱 간소화된 test error을 사용합니다. 이것을 **misclassification error**라고 부르며, 
이는 다음과 같습니다.

![J_test_new_error](/assets/images/ml/coursera/week6/J-test-new-error.png)

![J_test_new_full_function](/assets/images/ml/coursera/week6/J-test-new-full-function.png)

이것들이 hypothesis의 학습정도를 평가하는 **일반적인** 방법입니다.

## Model Selection and training Validation Test Sets

다음은 모델 선택시 몇차다항식이 가장 효과적인지 비교하고 가장 효과적인 차수의 다항식을 사용하는 방법을 알아보겠습니다.

이 때 가설함수가 몇차다항식이냐를 d 로 표현합니다. 3차다항식의 가설함수의 경우에 d는 3이 되겠죠?

우선, 다음과 같이 d=1 ~ d=10의 총 10개의 가설함수를 만듭니다.

![ten_hypothesis_function](/assets/images/ml/coursera/week6/ten-hypothesis-function.png)

그리고 각각의 가설함수에 대해 cost function을 minimize하는 $\theta$를 구합니다. 총 10개의 $\theta$가 나옵니다. 
**이 때 주의할 점은 $\theta$를 구하는 과정에서 training set의 데이터를 이용해야합니다.**

training set을 이용하여 구한 10개의 $\theta$를 이제는 $J_test(\theta)$에 넣어줍니다.

이 error function 중 가장 적은 error을 갖는 d를 채택하면 됩니다.

그러나, 이렇게 하면 모델이 training data를 memorise하게된다고 합니다. 실제로도 많이 발생한다고 합니다.

그래서 더 좋은 방법이 있습니다. 바로 Cross Validation set을 이용하는것인데요,
train, cross validation, test set의 비율을 60 : 20 : 20으로 분할하는 방법입니다.

![split_into_three_sets](/assets/images/ml/coursera/week6/split-into-three-sets.png)

기존과 같이 training set을 이용하여 학습을 시키고, 학습된 $\theta$들을 이용하여 각각의 **Cross Validation Error**를 구한 후
가장 낮은 **Cross Validation Error**을 갖는 d를 채택하는 방식입니다.

달라진 것이라면, test set은 전혀 건들지 않고, test error가 아닌 **cross validation error**을 이용하여 d를 구한다는 것입니다.


## Diagnosis - Bias vs Variance

다음으로, 집값을 예측했을 때 예측결과가 High Bias(underfitting)을 겪는지 High Variance(Overfitting)을 겪는지를 판별하는 법을 알아보겠습니다.
이 그래프는 d(다항식의 차수)에 따른 error을 나타내주는 그래프인데요, 이를 통해 성능이 잘 나오지 않는 모델이 underfitting을 겪는지, 혹은 overfitting겪는지 구별할 수 있습니다.

![Jcv_Jtrain_graph](/assets/images/ml/coursera/week6/Jcv-Jtrain-graph.png)
image

먼저, High Bias인 경우에는 $J_train$과 $J_cv$가 비슷한 높은 error을 갖습니다. 이 경우 underfit을 의심할 수 있습니다.
다음으로, High Variance인 경우에는 $J_cv$가 크고 $J_train$이 작은 error을 갖습니다. 이 경우 overfit이 의심되며, 충분히 자명해보입니다.

**이 그래프는 매우 중요한 그래프이며, overfit과 underfit의 차이를 드러냅니다.**


## Regularization and Bias/Variance

이제 $\lambda$를 고르는 방법에 대해 알아봅시다.
정규화 매개변수인 $\lambda$는 너무 높게 설정하면 underfit(high bias)이, 너무 작게 설정하면 overfit(high variance)이 발생합니다.

**따라서, 적정한 값의 $\lambda$를 설정하는 것은 좋은 결과를 내는데에 중요한 역할을 합니다.**

다음은 좋은 람다값을 찾는 방법입니다.

  - 다양한 $\lambda$를 설정한다. (보통 10개)
  - 각각의 $\lambda$들의 cost function J($\theta$)를 최소화하는 각각의 $\theta$들을 구한다.
  - 이 $\lambda$들에 매칭되는 $\theta$들을 $J_cv(\theta)$에 넣는다.
  - 최소의 $J_cv(\theta)$를 갖는 $\theta$를 찾아 $J_test(\theta)$에 넣는다.
  - 그 $\lambda$가 잘 골라졌는지 확인한다.

![lambda_selection](/assets/images/ml/coursera/week6/lambda-selection.png)

**이 때의 $J_train, J_cv, J_test$는 모두 regularization term이 삭제된 식임을 주의해주세요.**

$$ Regularization Term : + {\lambda \over 2m} \sum_{j=1}^{m}\theta_j^2$$


추가로, 다음 그래프는 $\lambda$에 따른 $J_cv$, $J_train$을 나타낸 것입니다. 위에서 본 그래프와 비교하면 좌우로 대칭이 된것을 확인할 수 있습니다.


![lambda_graph](/assets/images/ml/coursera/week6/lambda-graph.png)


## Learning Curves

### Learning Curve란?

learning curve란 성능 향상을, 혹은 알고리즘이 적합한지를 체크(algorithmic sanity check)할 때 주로 사용되는 그래프로,
$J_cv(\theta)$와 $J_train(\theta)$를 m에 따라 나타낸 것입니다.

이 그래프 또한 **high bias와 high variance를 구별하기 위해**, 또한 **데이터를 추가해야할지 여부**에 대해 자주 쓰이므로 잘 알아둘 필요가 있어보입니다.

![learning_curve_general](/assets/images/ml/coursera/week6/learning-curve-general.png)

일반적으로 $J_train$은 m이 작으면 fitting이 쉽지만, m이 커질수록 모든 data를 fitting하지 못하는 경우가 생기고, 이로인해 $J_train$ error가 증가합니다. 

반면, $J_cv$는 m들이 추가될수록 generalize되기 쉬워지며, 그에 따라 $J_cv$ error가 감소합니다.

### Learning Curve - High Bias

우선, High bias problem 즉 언더피팅의 경우를 살펴봅시다.

![learning_curve_underfit](/assets/images/ml/coursera/week6/learning-curve-underfit.png)

이 경우 그래프의 특징은 **높은 error에서의 수렴**입니다. 따라서 m의 증가가 error 감소에 아주 미미한 영향을 주게되며, **결과적으로 데이터양의 증가가 결과값 개선에 큰 도움을 주지 못합니다.**

이는 낮은 degree의 가설함수를 갖는 $J_cv$가 데이터수에 따른 영향을 적게받기 때문입니다.

다음 그림을 참고하시면 도움이 될 것 같습니다.

![learning_curve_underfit_m](/assets/images/ml/coursera/week6/learning-curve-underfit-m.png)


### Learning Curve - High Variance

다음은 High variance problem 즉 오버피팅의 경우입니다.

![learning_curve_overfit](/assets/images/ml/coursera/week6/learning-curve-overrfit.png)

오버피팅 러닝커브의 특징은 **일반적인 m에 대해 아주 높은 $J_cv$를 갖는다는 것**인데요, 이 때 x축(m)을 연장시켜주면 **낮은 error로 이동**하는것을 보실수 있습니다. 이와같이 m이 커질수록 $J_cv$가 점점 작아지는것을 확인할 수 있는데요, **결과적으로 많은 데이터가 결과값 개선에 도움을 줍니다.**

이는 높은 degree의 가설함수를 갖는 $J_cv$가 데이터를 잘 fitting하기 때문입니다. 오버피팅의 힘일까요?

역시 다음 그림을 참고해주세요.

![learning_curve_overfit_m](/assets/images/ml/coursera/week6/learning-curve-overfit-m.png)

이 그래프는 $J_cv$와 $J_train$간의 **Gap**이 또다른 특징입니다. 이를 통해 쉽게 오버피팅을 파악할 수 있습니다.


** 이 그래프들은 이상적인 경우의 그래프이며, 실제 데이터를 사용하다보면 훨씬 지저분한 그래프들을 만나게 될 것입니다. **


## What to do next (revisited)

이번 포스트의 처음에 우리의 결과값이 좋지 않을때 시도해볼 수 있는것들에 대해 나열했었습니다.
이제 그것들을 케이스별로 분류하면서 정리하겠습니다.

- 더 많은 데이터를 사용한다.  => fixes high variance
- feature의 수를 줄인다.  => fixes high variance
- feature의 수를 늘린다.  => fixes high bias
- polynomial feature을 추가한다.  => fixes high bias
- regularization parameter $\lambda$를 줄인다.  => fixes high bias
- regularization parameter $\lambda$를 키운다.  => fixes high variance


### Selecting a Network Architecture

뉴럴넷의 크기를 결정할 때 참고하면 좋을 상황들입니다. 각각 장단점이 존재합니다.

1. Small neural network

- 장점 : 연산량이 적다
- 단점 : parameter가 충분하지 않기 때문에 underfitting에 취약하다

2. Larger nueral network

- 장점 : 아무래도 좋은 결과값이 나온다.
- 단점 : 연산량이 많아지며, overfitting에 취약하다
- 특이사항 : overfitting은 regularization을 통해 해결할 수 있다.

3. 종합

  - 시작은 하나의 hidden layer로 시작하는 것이 좋다.
  - 주로 큰 nueral network를 고른 후 overfitting을 regularization을 통해 해결하는것이 더 좋다.


이상으로 6주차의 첫번째 섹션을 알아보았습니다. 결과값이 우리가 원한대로 나오지 않은 경우의 해결법에 대해 알아보았고,
다음 포스트에는 [Machine Learning System Design](/ml/MLcoursera-6-2/) 에 대해 알아보겠습니다.

감사합니다.

