---
layout: post
current: post
cover:  assets/images/ml.jpg
navigation: True
title: ML Coursera 1-2 Linear Regression with One Variable
date: 2019-05-28 10:00:00
tag :
  - machine learning
  - Coursera
class: post-template
subclass: 'post tag-getting-started'
author: hy38
---

> Coursera Andrew Ng 교수님의 ML 강의를 정리한 포스트입니다.

# Linear Regression with One Variable

머신러닝에 대한 간단한 소개를 마친 우리는 이제 선형회귀에 대하여 알아볼 것입니다. 우선 몇가지 사전 지식이 필요합니다. 선형회귀를 하기 위해서는 회기를 위한 데이터가 필요한데요, 이것을 **training set** 이라고 합니다. 이와 더불어 간단한 표기법들을 정리하면 수월할 것 같습니다.

## 간단한 표기법 정리
- m : training example의 갯수
- x's : input variables / features
- y : output variable

이전 포스트의 집값예측문제에서 그래프 상에 위치한 X들의 개수가 m이 될 것이고, 그 X의 x축 좌표를 x, y축 좌표를 y라고 지금은 편의상 인지하시면 됩니다. 

선형 회귀에서 우리의 궁극적인 목적은, 주어진 데이터에 가장 정확히 들어맞는 일직선을 찾는 것입니다. 그렇게 된다면 자기가 사고싶은 집크기에 따른 가격을 알아낼 수 있을테니 말입니다.(혹은 그 반대)

그렇기 때문에 우리는 hypothesis function이라는 가설함수 h(x)를 이용하여 데이터에 가장 근접한 line을 fitting합니다. 일직선이기 때문에 h(x)는 다음과 같은 모습을 띕니다.<br>
![hypothesis_function](/assets/images/ml/coursera/week1/hypothesis.png)

우리가 대학교 이전에 배운 일차함수들은 모두 y = ax + b 라고 배웠지만, 여기서는 사뭇 달라지기 때문에 참고하여 이해하면 좋을것 같습니다.
위의 h(x) 처럼 1개의 variable을 가진 선형회귀를 **univariate linear regression** 이라고 합니다. 말 그대로 단일변수 선형회귀 입니다. 이 역시 기억해두면 좋을 것 같습니다.

## Cost Function J

Cost function이라 함은 
우리가 regression을 할 때에 hypothesis function을 세우고 진행합니다. 그리고 hypothesis function의 세타값들을 변화시켜가며 최적의 선을 찾아냅니다. 그렇다면 이 최적의 선은 어떠한 방식으로 찾아내는 것일까요? 이를 찾아내는 함수가 따로 있습니다. 바로 **Cost Function**입니다. 임의의 세타값들을 대입한 경우 그 선이 데이터와 얼마나 잘 들어맞는가 (혹은 얼마나 어긋나는가)를 평가해준다고 생각하시면 되겠습니다. 우리는 이 Cost Function을 J를 이용하여 표기합니다.

Cost function은 모든 데이터에 대하여 각 데이터가 가설함수의 선과 얼마나 떨어져있는지를 계산하여 제곱을 합니다. 그리고 이들을 모두 더한 것을 2m 으로 나누어줍니다. (m은 위에서 언급했듯이 training example의 갯수입니다.) squared error cost function이라고도 불리는 이 함수를 수식화하면 다음과 같습니다.

![cost_function](/assets/images/ml/coursera/week1/cost-function.png)
2m으로 나누어줌으로써 각 데이터별 차이도의 평균을 구하게 되는 것입니다. 이 때 m이 아닌 2m으로 나누어 주는 이유는 추후에 이 식을 미분할 때의 편의성을 고려한 것입니다.

위의 내용을 바탕으로 정리하면, Cost function J는 우리가 만든 가설함수와 실제 데이터와의 차이도를 나타내주는 함수이므로, 그 차이도가 적을수록 실제 데이터에 들어맞는 가설함수를 세웠다고 할 수 있을 것입니다. 따라서 우리는 최적의 선을 만들기 위해서는 cost function J를 **최소화**해야 할 것입니다.

![cost_function_minimize](/assets/images/ml/coursera/week1/cost-function-minimize.png)

## Gradient Descent

그렇다면 이쯤에서 드는 의문이 cost function을 어떻게 **최소화할 것인가** 입니다.
우리는 Gradient Descent라는 경사하강법 알고리즘을 사용할 것인데, 이를 설명하기에 앞서서 cost function을 3D 모델화한 그림을 보겠습니다.
참고로 우리의 데이터는 다음 그림처럼 (1, 1), (2, 2), (3, 3)으로 가장 정확한 가설함수는 h(x) = 0 + 1x 일 것입니다.

![linear_data](/assets/images/ml/coursera/week1/linear-data.png)

![convex_3D_plot](/assets/images/ml/coursera/week1/convex-3D-plot.png)
위의 도자기(?)모양과 같은 그래프가 나오는데, 이를 통해 알수있는 것은 두가지 입니다.
 - 이 Cost function J는 최소값 1개를 갖는다. 
   -이 때에 J가 **convex**하다고 합니다.
 - 최소의 J를 갖는 h(x)의 두 variables의 값들을 알아낼 수 있다.

(추가적으로 contour plot을 이용하여 직관적인 이해를 할 수도 있지만, 생략하겠습니다.)

다시 본론으로 돌아와서, 위의 convex한 J에 대하여 최소가 되게하는 두 variable들(세타값들)을 어떻게 찾는지 설명하겠습니다.
우리는 **아무 점에서 시작**하여 산을 타고 내려가듯이 J가 낮은곳으로 이동을 할 것입니다. 구체적으로는 세타값들을 미세하게 변경해가며 J를 줄여나가는 것입니다. 이 과정을 더이상 내려갈 곳이 없는 **local minimum**에 수렴(도달)할 때까지 반복합니다.

![gradient_descent_3D](/assets/images/ml/coursera/week1/gradient-decent-3D.png)

다만, 주의할 것은 local minimum이 global minimum이 아닐 수 있다는 것입니다. 위 그림과 같이 여러개의 local minimum이 있는 경우에는 시작점이 조금만 달라져도 끝나는 점이 아예 달라질 수 있습니다.

### formal definition

이제 이 신기한 Gradient descent algorithm이 어떠한 수학적 방식으로 세타값들을 변경해나가는지 알아봅시다.
먼저, 수식을 보겠습니다. 이 수식을 수렴할 때까지 반복합니다.
**이 수식은 매우 중요합니다.**

![theta_update_rule](/assets/images/ml/coursera/week1/theta-update-rule.png)

**기본적인 미적분을 학습했다는 전제하에 진행하겠습니다.**
위 수식에서 처음 보는 문자 알파(alpha)가 나옵니다. 이는 **learning rate**을 나타내며, gradient descent를 얼마나 크게 크게 내려갈 것인지(혹은 작게 작게)를 결정하는 역할을 합니다.
대입연산자 := 를 이용하여 새로운 세타값을 업데이트하는 것 입니다. 이 때 **주의할 점**은, 세타값들을 다음과 같이 동시에 업데이트 해야한다는 것 입니다.

![theta_simultaneous_update](/assets/images/ml/coursera/week1/theta-simultaneous-update.png)

세타값들이 위 수식에서 계속 쓰이기 때문에, 하나의 세타를 먼저 업데이트 해버리면 다음 세타의 업데이트본을 구할 때 이미 업데이트된 세타값을 사용하여 다른 알고리즘이 되어버립니다. 잘못된 순서는 **1->3->2->4** 입니다.

위 식들에서 살펴볼 중요한 것들이 2가지 있습니다.
 - alpha값이 의미하는 것
   - alpha값이 너무 작으면, 세타가 줄어드는 속도가 느려집니다.
   - 반대로, 너무 크면 overshooting되는 경향이 있고, local minimum으로의 수렴에 실패하게 됩니다.
   - **따라서, 적당한 alpha값을 설정하는 것이 중요합니다.**
 - local minimum으로 수렴했을 경우의 세타 업데이트
   - 기울기가 0이기 때문에, 미분항은 0이고, alpha * 0 또한 0입니다.
   - 따라서 세타값 - 0은 세타값이므로, 기존의 세타값이 유지됩니다.

![theta_update_rule](/assets/images/ml/coursera/week1/theta-update-rule.png)

다시 이 수식으로 돌아와봅시다. 이 수식 자체를 받아들여보려는 노력을 해봅시다. 기존의 세타에서 J의 기울기 * alpha를 **뺄셈**을 하는 이유를 설명하겠습니다. 직관과 이해의 편의를 위해 세타_0 = 0인 다음과 같은 이차식을 참고해주세요.

![cost_function_graph](/assets/images/ml/coursera/week1/cost-function-graph.png)

위 그래프의 첫번째 시작점에서 그 위의 수식을 적용해봅시다. 미분한 값, 즉 기울기가 **양수**가 되므로 세타는 점점 **가운데**로 내려갑니다. 마찬가지로 두번째 시작점에서 적용해봅시다. 이때는 기울기가 음수이므로, 역시 세타가 점점 **가운데**로 내려가게됩니다. 따라서 결국에는 최솟값을 가지는 세타를 구하게 되는 셈입니다.

그렇다면 linear regression에서 cost function J는 항상 convex할까요?
답은 **Yes**입니다. 수학적으로 linear regression의 J함수는 볼록함수이기에 가운데의 최솟값으로 내려가는 것 입니다.
수학적으로는 볼록함수는 이계도함수가 양수인것을 뜻하는데요, 우리가 살펴본 cost function인 MSE(mean squared error)의 식을 두 번 미분한 것은 항상 양수이기 때문입니다. 자세한 것은 [여기](https://www.quora.com/Why-is-the-cost-function-in-linear-regression-a-convex-function-How-do-we-prove-it-mathematically)를 참고해보시면 좋을 것 같습니다! 


다음 포스트에서는 원래 Linear Algebra Review를 다룰 계획이었지만, 모두가 선형대수를 공부하였다는 가정하에 2주차로 넘어가 **Linear Regression with Multiple Variables**에 대해 다루겠습니다.

오탈자 / 오류 / 궁금사항 댓글로 남겨주시면 적극 반영하여 더욱 발전된 모습을 보여드리겠습니다. 궁금하신 것이 있으시면 언제든지 댓글을 남겨주시기 바랍니다. 감사합니다.
