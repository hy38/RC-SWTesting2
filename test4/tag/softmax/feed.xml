<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="https://hy38.github.io/tag/softmax/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://hy38.github.io/" rel="alternate" type="text/html" />
  <updated>2020-12-05T22:01:41+09:00</updated>
  <id>https://hy38.github.io/tag/softmax/feed.xml</id>

  
  
  

  
    <title type="html">HY38’s IT Blog | </title>
  

  
    <subtitle>b Ha p</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">D2L 3 - Softmax Regression</title>
      <link href="https://hy38.github.io/D2L-3-softmax-regression" rel="alternate" type="text/html" title="D2L 3 - Softmax Regression" />
      <published>2019-06-15T19:00:00+09:00</published>
      <updated>2019-06-15T19:00:00+09:00</updated>
      <id>https://hy38.github.io/D2L-3-softmax-regression</id>
      <content type="html" xml:base="https://hy38.github.io/D2L-3-softmax-regression">&lt;blockquote&gt;
  &lt;p&gt;This is a post that I organized while studying &lt;a href=&quot;https://d2l.ai/index.html&quot; target=&quot;_blank&quot;&gt;D2L(Dive into Deep Learning)&lt;/a&gt; exercises.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;what-is-softmax&quot;&gt;What is Softmax&lt;/h4&gt;

&lt;p&gt;Softmax is a procedure producing &lt;strong&gt;probabilities&lt;/strong&gt; that maximize the &lt;strong&gt;likelihood&lt;/strong&gt; of the observed data.&lt;/p&gt;

&lt;p&gt;We define the label with the probability of the data we observe.&lt;/p&gt;

&lt;h3 id=&quot;cross-entropy-loss&quot;&gt;Cross Entropy Loss&lt;/h3&gt;

&lt;p&gt;We can think of the cross-entropy classification objective in two ways: (i) as maximizing the likelihood of the observed data; and (ii) as minimizing our surprise (and thus the number of bits) required to communicate the labels.&lt;/p&gt;

&lt;h4 id=&quot;negative-log-likelihood-loss-function&quot;&gt;Negative Log-likelihood Loss Function&lt;/h4&gt;

&lt;p&gt;What we want is to &lt;strong&gt;maximize&lt;/strong&gt; the likelihood of data. And the expression is below.&lt;/p&gt;

&lt;p&gt;$P(Y \mid X) = \prod_{i=1}^n P(y^{(i)} \mid x^{(i)})$&lt;/p&gt;

&lt;p&gt;For the convenience of math, we plug (natural) logarithm.&lt;/p&gt;

&lt;p&gt;By plugging log to this expression, &lt;code class=&quot;highlighter-rouge&quot;&gt;product&lt;/code&gt; gets replaced by &lt;code class=&quot;highlighter-rouge&quot;&gt;sum&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Since optimizers typically do minimize a function, minimizing the expression is more advantageous rather than maximizing.&lt;/p&gt;

&lt;p&gt;And fortunately, logarithm is a monotonically increasing function.&lt;/p&gt;

&lt;p&gt;That is the reason we use &lt;strong&gt;NEGATIVE&lt;/strong&gt; Log-likelihood loss function!&lt;/p&gt;

&lt;h3 id=&quot;exercise--softmax-regression&quot;&gt;Exercise : Softmax Regression&lt;/h3&gt;

&lt;h6 id=&quot;1-show-that-the-kullback-leibler-divergence-dpq-is-nonnegative-for-all-distributions-p-and-q-hint-use-jensens-inequality-ie-use-the-fact-that-logx-is-a-convex-function&quot;&gt;1) Show that the Kullback-Leibler divergence $D(p||q)$ is nonnegative for all distributions $p$ and $q$. Hint: use Jensen’s inequality, i.e., use the fact that $−logx$ is a convex function.&lt;/h6&gt;

&lt;h6 id=&quot;2-show-that-log-sum_j-expo_j-is-a-convex-function-in-o&quot;&gt;2) Show that $\log \sum_j \exp(o_j)$ is a convex function in $o$.&lt;/h6&gt;

&lt;h6 id=&quot;3-we-can-explore-the-connection-between-exponential-families-and-the-softmax-in-some-more-depth&quot;&gt;3) We can explore the connection between exponential families and the softmax in some more depth&lt;/h6&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Compute the second derivative of the cross-entropy loss$l(y,\hat{y})$ for the softmax.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute the variance of the distribution given by $softmax(o)$ and show that it matches the second derivative computed above.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;4-assume-that-we-have-three-classes-which-occur-with-equal-probability-ie-the-probability-vector-is-dfrac13-dfrac13-dfrac13&quot;&gt;4) Assume that we have three classes which occur with equal probability, i.e., the probability vector is $(\dfrac{1}{3}, \dfrac{1}{3}, \dfrac{1}{3})$.&lt;/h6&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;What is the problem if we try to design a binary code for it? Can we match the entropy lower bound on the number of bits?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Can you design a better code. Hint: what happens if we try to encode two independent observations? What if we encode  n  observations jointly?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;5-softmax-is-a-misnomer-for-the-mapping-introduced-above-but-everyone-in-deep-learning-uses-it-the-real-softmax-is-defined-as-mathrmrealsoftmaxa-b--log-expa--expb&quot;&gt;5) Softmax is a misnomer for the mapping introduced above (but everyone in deep learning uses it). The real softmax is defined as $\mathrm{RealSoftMax}(a, b) = \log (\exp(a) + \exp(b))$.&lt;/h6&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Prove that $\mathrm{RealSoftMax}(a, b) &amp;gt; \mathrm{max}(a, b)$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Prove that this holds for $\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b)$, provided that $\lambda &amp;gt; 0$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Show that for $\lambda \to \infty$ we have $\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b) \to \mathrm{max}(a, b)$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;What does the soft-min look like?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Extend this to more than two numbers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Why do we minimize NLL &lt;a href=&quot;https://stats.stackexchange.com/questions/141087/why-do-we-minimize-the-negative-likelihood-if-it-is-equivalent-to-maximization-o&quot; target=&quot;_blank&quot;&gt;https://stats.stackexchange.com/questions/141087/why-do-we-minimize-the-negative-likelihood-if-it-is-equivalent-to-maximization-o&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;about MLE &lt;a href=&quot;https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1&quot; target=&quot;_blank&quot;&gt;https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;about Entropy &lt;a href=&quot;https://hyunw.kim/blog/2017/10/14/Entropy.html&quot; target=&quot;_blank&quot;&gt;https://hyunw.kim/blog/2017/10/14/Entropy.html&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Sanghyun Park</name>
        
        
      </author>

      

      
        <category term="deep learning" />
      
        <category term="softmax" />
      

      
        <summary type="html">This is a post that I organized while studying D2L(Dive into Deep Learning) exercises.</summary>
      

      
      
    </entry>
  
</feed>
